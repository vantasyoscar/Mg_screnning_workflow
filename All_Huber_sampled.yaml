# a simple example config file

# Two folders will be used during the training: 'root'/process and 'root'/'run_name'
# run_name contains logfiles and saved models
# process contains processed data sets
# if 'root'/'run_name' exists, 'root'/'run_name'_'year'-'month'-'day'-'hour'-'min'-'s' will be used instead.
root: ./MP_GGA
run_name: Huber_sampled
seed: 2233                                                                         # model seed
dataset_seed: 2333                                                                 # data set seed
append: true                                                                      # set true if a restarted run should append to the previous log file

# see https://arxiv.org/abs/2304.10061 for discussion of numerical precision
default_dtype: float64
model_dtype: float32
allow_tf32: true    # consider setting to false if you plan to mix training/inference over any devices that are not NVIDIA Ampere or later

model_builders:
  - SimpleIrrepsConfig
  - EnergyModel
  - PerSpeciesRescale
  - StressForceOutput
  - RescaleEnergyEtc

#initial_model_state: /my/previous/early-stopped/training/run/last_model.pth

# network
r_max: 5                                                                         # cutoff radius in length units, here Angstrom, this is an important hyperparamter to scan
num_layers: 4                                                                     # number of interaction blocks, we find 3-5 to work best
#l_max: 2                                                                          # the maximum irrep order (rotation order) for the network's features, l=2 is accurate but slower, l=1 if you want to be faster but less accurte
#parity: true                                                                      # whether to include features with odd mirror parity; often turning parity off gives equally good results but faster networks, so do consider this
#num_features: 64                                                                  # the multiplicity of the features, 32 is a good default for accurate network, if you want to be more accurate, go larger, if you want to be faster, go lower
chemical_embedding_irreps_out: 64x0e                   
feature_irreps_hidden: 128x0e + 64x1e + 32x2e   
irreps_edge_sh: 1x0e+1x1e+1x2e                               
conv_to_output_hidden_irreps_out: 32x0e 

nonlinearity_type: gate                                                           # may be 'gate' or 'norm', 'gate' is recommended



# scalar nonlinearities to use â€” available options are silu, ssp (shifted softplus), tanh, and abs.
# Different nonlinearities are specified for e (even) and o (odd) parity;
# note that only tanh and abs are correct for o (odd parity)
# silu typically works best for even 
nonlinearity_scalars:
  e: silu
  o: tanh

nonlinearity_gates:
  e: silu
  o: tanh

# radial network basis
num_basis: 8                                                                      # number of basis functions used in the radial basis, 8 usually works best
BesselBasis_trainable: true                                                       # set true to train the bessel weights
PolynomialCutoff_p: 6                                                             # p-exponent used in polynomial cutoff function, smaller p corresponds to stronger decay with distance

# radial network
invariant_layers: 2                                                               # number of radial layers, usually 1-3 works best, smaller is faster
invariant_neurons: 64                                                             # number of hidden neurons in radial function, smaller is faster
avg_num_neighbors: auto                                                           # number of neighbors to divide by, null => no normalization, auto computes it based on dataset 
use_sc: true                                                                      # use self-connection or not, usually gives big improvement

# data set
# the keys used need to be stated at least once in key_mapping, npz_fixed_field_keys or npz_keys
# key_mapping is used to map the key in the npz file to the NequIP default values (see data/_key.py)
# all arrays are expected to have the shape of (nframe, natom, ?) except the fixed fields
# note that if your data set uses pbc, you need to also pass an array that maps to the nequip "pbc" key
dataset: ase
dataset_file_name: ../MPdata/MP_all_rmin5_sampled_fmax5.extxyz
ase_args:
  format: extxyz
chemical_symbol_to_type:
  H: 0
  He: 1
  Li: 2
  Be: 3
  B: 4
  C: 5
  N: 6
  O: 7
  F: 8
  Ne: 9
  Na: 10
  Mg: 11
  Al: 12
  Si: 13
  P: 14
  S: 15
  Cl: 16
  Ar: 17
  K: 18
  Ca: 19
  Sc: 20
  Ti: 21
  V: 22
  Cr: 23
  Mn: 24
  Fe: 25
  Co: 26
  Ni: 27
  Cu: 28
  Zn: 29
  Ga: 30
  Ge: 31
  As: 32
  Se: 33
  Br: 34
  Kr: 35
  Rb: 36
  Sr: 37
  Y: 38
  Zr: 39
  Nb: 40
  Mo: 41
  Tc: 42
  Ru: 43
  Rh: 44
  Pd: 45
  Ag: 46
  Cd: 47
  In: 48
  Sn: 49
  Sb: 50
  Te: 51
  I: 52
  Xe: 53
  Cs: 54
  Ba: 55
  La: 56
  Ce: 57
  Pr: 58
  Nd: 59
  Pm: 60
  Sm: 61
  Eu: 62
  Gd: 63
  Tb: 64
  Dy: 65
  Ho: 66
  Er: 67
  Tm: 68
  Yb: 69
  Lu: 70
  Hf: 71
  Ta: 72
  W: 73
  Re: 74
  Os: 75
  Ir: 76
  Pt: 77
  Au: 78
  Hg: 79
  Tl: 80
  Pb: 81
  Bi: 82
  Po: 83
  At: 84
  Rn: 85
  Fr: 86
  Ra: 87
  Ac: 88
  Th: 89
  Pa: 90
  U: 91
  Np: 92
  Pu: 93



# logging
wandb: false                                                                        # we recommend using wandb for logging
#wandb_project: toluene-example                                                     # project name used in wandb

verbose: info                                                                      # the same as python logging, e.g. warning, info, debug, error; case insensitive
log_batch_freq: 1000                                                                # batch frequency, how often to print training errors withinin the same epoch
log_epoch_freq: 1                                                                  # epoch frequency, how often to print 
save_checkpoint_freq: -1                                                           # frequency to save the intermediate checkpoint. no saving of intermediate checkpoints when the value is not positive.
save_ema_checkpoint_freq: -1                                                       # frequency to save the intermediate ema checkpoint. no saving of intermediate checkpoints when the value is not positive.

# training
n_train: 500000                                                                   # number of training data
n_val:   67815          #456204 for Mg    2008494 for GGA                          # number of validation data
learning_rate: 0.002                                                               # learning rate, we found values between 0.01 and 0.005 to work best - this is often one of the most important hyperparameters to tune
batch_size: 8                                                                      # batch size, we found it important to keep this small for most applications including forces (1-5); for energy-only training, higher batch sizes work better
validation_batch_size: 32                                                          # batch size for evaluating the model during validation. This does not affect the training results, but using the highest value possible (<=n_val) without running out of memory will speed up your training.
max_epochs: 100000                                                                 # stop training after _ number of epochs, we set a very large number, as e.g. 1million and then just use early stopping and not train the full number of epochs
train_val_split: random                                                            # can be random or sequential. if sequential, first n_train elements are training, next n_val are val, else random, usually random is the right choice
shuffle: true                                                                      # if true, the data loader will shuffle the data, usually a good idea
metrics_key: validation_loss                                                       # metrics used for scheduling and saving best model. Options: `set`_`quantity`, set can be either "train" or "validation, "quantity" can be loss or anything that appears in the validation batch step header, such as f_mae, f_rmse, e_mae, e_rmse
use_ema: true                                                                      # if true, use exponential moving average on weights for val/test, usually helps a lot with training, in particular for energy errors
ema_decay: 0.99                                                                    # ema weight, typically set to 0.99 or 0.999
ema_use_num_updates: true                                                          # whether to use number of updates when computing averages
report_init_validation: true                                                       # if True, report the validation error for just initialized model

# early stopping based on metrics values.
early_stopping_patiences:                                                          # stop early if a metric value stopped decreasing for n epochs
  validation_loss: 50

early_stopping_lower_bounds:                                                       # stop early if a metric value is lower than the bound
  LR: 5.0e-6

early_stopping_upper_bounds:                                                       # stop early if the training appears to have exploded
  validation_loss: 1.0e+5

# loss function

loss_coeffs:                                                                    
  #stress: 1
  forces:                                                                        # if using PerAtomMSELoss, a default weight of 1:1 on each should work well
    - 5
    - SmoothL1Loss
  total_energy:                                                                    
    - 1
    - PerAtomSmoothL1Loss

metrics_components:
  - - forces                               # key 
    - mae                                  # "rmse" or "mae"
  - - forces
    - rmse
  - - forces
    - mae
    - PerSpecies: True                     # if true, per species contribution is counted separately
      report_per_component: False          # if true, statistics on each component (i.e. fx, fy, fz) will be counted separately
  - - forces                                
    - rmse                                  
    - PerSpecies: True                     
      report_per_component: False    
  - - total_energy
    - mae    
  - - total_energy
    - mae
    - PerAtom: True                        # if true, energy is normalized by the number of atoms

# optimizer, may be any optimizer defined in torch.optim
# the name `optimizer_name`is case sensitive
# IMPORTANT: for NequIP (not for Allegro), we find that in most cases AMSGrad strongly improves
# out-of-distribution generalization over Adam. We highly recommed trying both AMSGrad (by setting
# optimizer_amsgrad: true) and Adam (by setting optimizer_amsgrad: false)
optimizer_name: Adam                                                             
optimizer_amsgrad: true

# lr scheduler, currently only supports the two options listed in full.yaml, i.e. on-pleteau and cosine annealing with warm restarts, if you need more please file an issue
# here: on-plateau, reduce lr by factory of lr_scheduler_factor if metrics_key hasn't improved for lr_scheduler_patience epoch
lr_scheduler_name: ReduceLROnPlateau
lr_scheduler_patience: 3
lr_scheduler_factor: 0.6

# we provide a series of options to shift and scale the data
# these are for advanced use and usually the defaults work very well
# the default is to scale the atomic energy and forces by scaling them by the force standard deviation and to shift the energy by the mean atomic energy
# in certain cases, it can be useful to have a trainable shift/scale and to also have species-dependent shifts/scales for each atom

# initial atomic energy shift for each species. default to the mean of per atom energy. Optional
# the value can be a constant float value, an array for each species, or a string that defines a statistics over the training dataset
# if numbers are explicitly provided, they must be in the same energy units as the training data
per_species_rescale_shifts: dataset_per_atom_total_energy_mean

# initial atomic energy scale for each species. Optional.
# the value can be a constant float value, an array for each species, or a string
# if numbers are explicitly provided, they must be in the same energy units as the training data
per_species_rescale_scales: dataset_per_atom_total_energy_std
